{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndianBatsModel - Real-World Inference Pipeline\n",
    "\n",
    "This notebook runs the trained model on **raw, uncurated audio files**.\n",
    "Unlike the test notebook, this pipeline handles:\n",
    "1.  **Detection**: Finding potential bat calls in long recordings (ignoring silence/noise).\n",
    "2.  **Classification**: Predicting the species for each detected call.\n",
    "3.  **Filtering**: Ignoring low-confidence predictions.\n",
    "\n",
    "**Use this for:** Field recordings or files where you don't know if/where the bats are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea620c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Environment\n",
    "!git clone https://github.com/Quarkisinproton/IndianBatsModel.git\n",
    "!pip install librosa pyyaml pandas matplotlib scikit-learn tqdm python-docx scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Modules\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.signal import butter, filtfilt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define working directory\n",
    "WORK_DIR = '/kaggle/working'\n",
    "\n",
    "\n",
    "# Add repo path\n",
    "REPO_DIR = os.path.join(WORK_DIR, 'IndianBatsModel')\n",
    "SRC_DIR = os.path.join(REPO_DIR, 'src')\n",
    "if REPO_DIR not in sys.path: sys.path.append(REPO_DIR)\n",
    "if SRC_DIR not in sys.path: sys.path.append(SRC_DIR)\n",
    "\n",
    "# Import project modules\n",
    "try:\n",
    "    from MainShitz.models.cnn_with_features import CNNWithFeatures\n",
    "    from MainShitz.data_prep.wombat_to_spectrograms import make_mel_spectrogram\n",
    "    from MainShitz.data_prep.extract_end_frequency import compute_end_frequency\n",
    "    print(\"Imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuration\n",
    "\n",
    "# --- INPUTS ---\n",
    "# Path to your trained model\n",
    "MODEL_PATH = '/kaggle/working/models/bat_fused_best.pth'\n",
    "\n",
    "# Path to folder containing RAW .wav files\n",
    "INPUT_AUDIO_DIR = '/kaggle/input/your-test-audio-folder' \n",
    "\n",
    "# --- DETECTION SETTINGS ---\n",
    "SAMPLE_RATE = 250000  # Typical for bat recorders (adjust if needed)\n",
    "MIN_FREQ = 15000      # High-pass filter cutoff (15kHz) to remove wind/noise\n",
    "ENERGY_THRESH = 0.02  # RMS energy threshold to trigger detection\n",
    "MIN_DURATION = 0.01   # Minimum call duration (seconds)\n",
    "PAD_DURATION = 0.05   # Padding around detected call (seconds)\n",
    "CONFIDENCE_THRESH = 70.0 # Only report predictions > 70% confidence\n",
    "\n",
    "# --- MODEL SETTINGS ---\n",
    "NUM_CLASSES = 2       # Must match your trained model\n",
    "CLASS_NAMES = ['pip-ceylonicusbat-species', 'pip-tenuisbat-species'] # Ensure correct order!\n",
    "\n",
    "# Auto-detect model in /kaggle/input\n",
    "found_models = []\n",
    "for root, dirs, files in os.walk('/kaggle/input'):\n",
    "    for file in files:\n",
    "        if file.endswith('.pth'):\n",
    "            found_models.append(os.path.join(root, file))\n",
    "if found_models:\n",
    "    MODEL_PATH = found_models[0]\n",
    "    print(f\"Auto-detected model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Detection & Processing Functions\n",
    "\n",
    "def bandpass_filter(y, sr, low=15000, high=120000, order=4):\n",
    "    \"\"\"Lightweight Butterworth band-pass to remove low rumble and out-of-band noise.\"\"\"\n",
    "    if y is None or len(y) == 0:\n",
    "        return y\n",
    "    nyq = 0.5 * sr\n",
    "    low = max(1.0, min(low, nyq * 0.9))\n",
    "    high = min(high, nyq * 0.99)\n",
    "    if high <= low:\n",
    "        return y\n",
    "    b, a = butter(order, [low / nyq, high / nyq], btype='band')\n",
    "    return filtfilt(b, a, y)\n",
    "\n",
    "\n",
    "def save_spectrogram_like_example(\n",
    "    y_seg,\n",
    "    sr,\n",
    "    out_path,\n",
    "    *,\n",
    "    fmin=15000,\n",
    "    fmax=120000,\n",
    "    n_fft=2048,\n",
    "    hop_length=256,\n",
    "    n_mels=256,\n",
    "    cmap='viridis',\n",
    "    db_floor=-80.0,\n",
    "    dpi=150,\n",
    "):\n",
    "    \"\"\"Save a clean, axis-less spectrogram PNG similar to the provided example.\"\"\"\n",
    "    if y_seg is None or len(y_seg) < n_fft:\n",
    "        return False\n",
    "\n",
    "    # cap fmax to Nyquist\n",
    "    nyq = (sr * 0.5) - 1\n",
    "    fmax = min(float(fmax), float(nyq))\n",
    "    if fmax <= fmin:\n",
    "        fmax = float(nyq)\n",
    "\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y_seg,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "        power=2.0,\n",
    "    )\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    S_db = np.clip(S_db, db_floor, 0.0)\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(S_db, origin='lower', aspect='auto', cmap=cmap, vmin=db_floor, vmax=0.0, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.margins(0)\n",
    "    plt.savefig(out_path, bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "    plt.close()\n",
    "    return True\n",
    "\n",
    "\n",
    "def detect_events(y, sr, min_freq=15000, threshold=0.01, min_dur=0.01, pad=0.05):\n",
    "    \"\"\"Finds segments of interest in the audio based on energy in high frequencies.\"\"\"\n",
    "    # Denoise: band-pass around bat band\n",
    "    high_cut = min(120000, (sr * 0.5) - 1000) if sr else 120000\n",
    "    y = bandpass_filter(y, sr, low=min_freq, high=high_cut)\n",
    "\n",
    "    # 1. High-pass filter (legacy STFT masking for event detection)\n",
    "    S = librosa.stft(y, n_fft=2048, hop_length=512)\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=2048)\n",
    "\n",
    "    # Zero out frequencies below min_freq\n",
    "    S_filtered = S.copy()\n",
    "    mask = freqs < min_freq\n",
    "    S_filtered[mask, :] = 0\n",
    "\n",
    "    # 2. Calculate RMS energy profile of filtered signal\n",
    "    rms = librosa.feature.rms(S=S_filtered, frame_length=2048, hop_length=512)[0]\n",
    "    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=512)\n",
    "\n",
    "    # 3. Thresholding\n",
    "    is_active = rms > threshold\n",
    "\n",
    "    # 4. Group into segments\n",
    "    events = []\n",
    "    start = None\n",
    "    for i, active in enumerate(is_active):\n",
    "        if active and start is None:\n",
    "            start = times[i]\n",
    "        elif not active and start is not None:\n",
    "            end = times[i]\n",
    "            if (end - start) >= min_dur:\n",
    "                events.append((max(0, start - pad), min(librosa.get_duration(y=y, sr=sr), end + pad)))\n",
    "            start = None\n",
    "\n",
    "    # Handle case where file ends while active\n",
    "    if start is not None:\n",
    "        end = times[-1]\n",
    "        if (end - start) >= min_dur:\n",
    "            events.append((max(0, start - pad), end))\n",
    "\n",
    "    # Merge overlapping segments\n",
    "    if not events:\n",
    "        return []\n",
    "\n",
    "    merged = []\n",
    "    curr_start, curr_end = events[0]\n",
    "    for next_start, next_end in events[1:]:\n",
    "        if next_start <= curr_end:\n",
    "            curr_end = max(curr_end, next_end)\n",
    "        else:\n",
    "            merged.append((curr_start, curr_end))\n",
    "            curr_start, curr_end = next_start, next_end\n",
    "    merged.append((curr_start, curr_end))\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def prepare_input(y, sr, start, end):\n",
    "    \"\"\"Extracts segment, generates spectrogram and features for the model.\"\"\"\n",
    "    # Extract audio\n",
    "    start_sample = int(start * sr)\n",
    "    end_sample = int(end * sr)\n",
    "    y_seg = y[start_sample:end_sample]\n",
    "\n",
    "    if len(y_seg) < 512:\n",
    "        return None, None, None, None, None  # Too short\n",
    "\n",
    "    # Denoise segment\n",
    "    high_cut = min(120000, (sr * 0.5) - 1000) if sr else 120000\n",
    "    y_seg = bandpass_filter(y_seg, sr, low=15000, high=high_cut)\n",
    "\n",
    "    # 1. Spectrogram\n",
    "    S_db = make_mel_spectrogram(y_seg, sr)\n",
    "    energy_db_max = float(S_db.max())\n",
    "\n",
    "    # Convert to Image (normalize to 0-255 like training)\n",
    "    S_min, S_max = S_db.min(), S_db.max()\n",
    "    S_norm = (S_db - S_min) / (S_max - S_min + 1e-8)\n",
    "\n",
    "    # Apply Magma Colormap (matches training data)\n",
    "    S_colored = plt.get_cmap('magma')(S_norm)\n",
    "\n",
    "    # Convert to uint8 RGB (drop alpha)\n",
    "    S_img = (S_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "    # Flip vertically so low frequency is at the bottom\n",
    "    S_img = np.flipud(S_img)\n",
    "\n",
    "    img = Image.fromarray(S_img)\n",
    "\n",
    "    # Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img_tensor = transform(img)\n",
    "\n",
    "    # 2. Features (End Frequency)\n",
    "    end_freq = compute_end_frequency(y, sr, start, end)\n",
    "    if np.isnan(end_freq):\n",
    "        end_freq = 0.0\n",
    "    feat_tensor = torch.tensor([end_freq], dtype=torch.float32)\n",
    "\n",
    "    return img_tensor, feat_tensor, img, y_seg, energy_db_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef8c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Initialize model structure\n",
    "    model = CNNWithFeatures(num_classes=NUM_CLASSES, numeric_feat_dim=1, pretrained=False)\n",
    "    \n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(f\"Loading {MODEL_PATH}...\")\n",
    "        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "        \n",
    "        if isinstance(checkpoint, torch.nn.Module):\n",
    "            model = checkpoint\n",
    "        elif isinstance(checkpoint, dict):\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"Model loaded!\")\n",
    "    else:\n",
    "        print(\"Model file not found!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8cdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Run Inference Loop\n",
    "from IPython.display import display\n",
    "\n",
    "# Output directory for spectrograms\n",
    "SPECTROGRAM_DIR = os.path.join(WORK_DIR, 'inference_spectrograms')\n",
    "os.makedirs(SPECTROGRAM_DIR, exist_ok=True)\n",
    "print(f'Saving spectrograms to {SPECTROGRAM_DIR}...')\n",
    "\n",
    "# Thresholds\n",
    "LOW_ENERGY_THRESH_DB = -55.0\n",
    "\n",
    "# Spectrogram style settings (matches your example)\n",
    "SPEC_CMAP = 'viridis'\n",
    "SPEC_DB_FLOOR = -80.0\n",
    "SPEC_FMIN = 15000\n",
    "SPEC_FMAX = 120000\n",
    "\n",
    "# Word report\n",
    "doc = Document()\n",
    "doc.add_heading('Bat Species Inference Report', 0)\n",
    "\n",
    "results = []\n",
    "\n",
    "def format_time(seconds):\n",
    "    return str(timedelta(seconds=float(seconds)))\n",
    "\n",
    "# Find audio files\n",
    "audio_files = []\n",
    "if os.path.exists(INPUT_AUDIO_DIR):\n",
    "    for root, dirs, files in os.walk(INPUT_AUDIO_DIR):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(('.wav', '.mp3', '.flac')):\n",
    "                audio_files.append(os.path.join(root, f))\n",
    "else:\n",
    "    print(f\"Input directory {INPUT_AUDIO_DIR} does not exist.\")\n",
    "\n",
    "print(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "for audio_path in tqdm(audio_files):\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "        \n",
    "        # Detect events\n",
    "        events = detect_events(y, sr, min_freq=MIN_FREQ, threshold=ENERGY_THRESH, min_dur=MIN_DURATION, pad=PAD_DURATION)\n",
    "        \n",
    "        if not events:\n",
    "            results.append({\n",
    "                'filename': os.path.basename(audio_path),\n",
    "                'start': '-', 'end': '-',\n",
    "                'prediction': 'No Detection',\n",
    "                'confidence': 0.0\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        # Process each event\n",
    "        for start, end in events:\n",
    "            img_t, feat_t, pil_img, y_seg, energy_db_max = prepare_input(y, sr, start, end)\n",
    "            if img_t is None:\n",
    "                print(f\"  Skipping too-short segment [{format_time(start)} - {format_time(end)}]\")\n",
    "                continue\n",
    "            \n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                img_batch = img_t.unsqueeze(0).to(device)\n",
    "                feat_batch = feat_t.unsqueeze(0).to(device)\n",
    "                \n",
    "                output = model(img_batch, feat_batch)\n",
    "                probs = torch.nn.functional.softmax(output, dim=1)\n",
    "                conf, pred_idx = torch.max(probs, 1)\n",
    "            confidence_pct = conf.item() * 100\n",
    "            pred_class = CLASS_NAMES[pred_idx.item()]\n",
    "            \n",
    "            # Reject obvious noise by low energy or low confidence\n",
    "            if energy_db_max < LOW_ENERGY_THRESH_DB:\n",
    "                final_pred = \"Noise/LowEnergy\"\n",
    "                confidence_pct = 0.0\n",
    "            else:\n",
    "                final_pred = pred_class if confidence_pct >= CONFIDENCE_THRESH else \"Uncertain\"\n",
    "\n",
    "            # Save styled spectrogram (looks like your example)\n",
    "            spec_filename = f\"{Path(audio_path).stem}_{start:.2f}_{end:.2f}_{final_pred}.png\"\n",
    "            spec_path = os.path.join(SPECTROGRAM_DIR, spec_filename)\n",
    "            _ok = save_spectrogram_like_example(\n",
    "                y_seg,\n",
    "                sr,\n",
    "                spec_path,\n",
    "                fmin=SPEC_FMIN,\n",
    "                fmax=SPEC_FMAX,\n",
    "                cmap=SPEC_CMAP,\n",
    "                db_floor=SPEC_DB_FLOOR,\n",
    "            )\n",
    "\n",
    "            # Display inline if confident or marked noise\n",
    "            if final_pred != \"Uncertain\":\n",
    "                print(f\"\\nDetected: {final_pred} ({confidence_pct:.1f}%) at {start:.2f}-{end:.2f}s in {os.path.basename(audio_path)}\")\n",
    "                if _ok:\n",
    "                    display(Image.open(spec_path))\n",
    "\n",
    "            results.append({\n",
    "                'filename': os.path.basename(audio_path),\n",
    "                'start': f\"{start:.2f}\",\n",
    "                'end': f\"{end:.2f}\",\n",
    "                'prediction': final_pred,\n",
    "                'confidence': f\"{confidence_pct:.1f}\",\n",
    "                'energy_db_max': f\"{energy_db_max:.1f}\"\n",
    "            })\n",
    "\n",
    "            # Add to Word report\n",
    "            p = doc.add_paragraph()\n",
    "            p.add_run(f\"File: {os.path.basename(audio_path)}\\n\").bold = True\n",
    "            p.add_run(f\"Segment: {format_time(start)} - {format_time(end)}\\n\")\n",
    "            p.add_run(f\"Prediction: {final_pred}\\n\")\n",
    "            p.add_run(f\"Confidence: {confidence_pct:.1f}%\\n\")\n",
    "            p.add_run(f\"Max Energy (dB): {energy_db_max:.1f}\\n\")\n",
    "            if _ok:\n",
    "                doc.add_picture(spec_path, width=Inches(6))\n",
    "            doc.add_paragraph('-' * 50)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "# Display Results and save report\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nInference Results:\")\n",
    "print(df.to_string())\n",
    "\n",
    "report_path = os.path.join(WORK_DIR, 'Inference_Report.docx')\n",
    "doc.save(report_path)\n",
    "print(f\"Report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33eda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save Results for Manual Verification\n",
    "# Since we don't know the true species for these files, we save the predictions\n",
    "# to a CSV file. You can give this file to an expert for verification.\n",
    "\n",
    "if not df.empty:\n",
    "    output_csv_path = os.path.join(WORK_DIR, 'inference_results.csv')\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\nSUCCESS: Results saved to: {output_csv_path}\")\n",
    "    print(\"Columns: filename, start, end, prediction, confidence, energy_db_max\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Download this file from the 'Output' section of your Kaggle notebook.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Preview again\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"No detections found. No CSV generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Evaluate Accuracy against Ground Truth\n",
    "# PERU3.txt does NOT include a filename column.\n",
    "# So evaluation only makes sense if:\n",
    "#   - you ran inference on the matching audio file (e.g. PERU3.wav), OR\n",
    "#   - you select which `df['filename']` to evaluate.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Choose which audio file in df to evaluate ----\n",
    "# If you ran inference on multiple files, set this explicitly (exact basename):\n",
    "EVAL_AUDIO_FILENAME = None  # e.g. \"PERU3.wav\"\n",
    "\n",
    "# ---- Locate ground truth file ----\n",
    "GT_CANDIDATES = [\n",
    "    os.path.join(REPO_DIR, 'data', 'PERU3.txt'),\n",
    "    os.path.join(WORK_DIR, 'PERU3.txt'),\n",
    "    '/kaggle/input/indian-bats-data/PERU3.txt',\n",
    "    '../data/PERU3.txt',\n",
    "]\n",
    "\n",
    "GT_FILE_PATH = next((p for p in GT_CANDIDATES if os.path.exists(p)), None)\n",
    "\n",
    "if GT_FILE_PATH is None:\n",
    "    print(\"Ground truth file (PERU3.txt) not found. Skipping evaluation.\")\n",
    "    print(f\"Checked locations: {GT_CANDIDATES}\")\n",
    "else:\n",
    "    print(f\"Loading ground truth from: {GT_FILE_PATH}\")\n",
    "\n",
    "    if 'df' not in globals() or df is None or df.empty:\n",
    "        print(\"Results dataframe `df` is empty/not found. Run inference first.\")\n",
    "    else:\n",
    "        # ---- Decide which filename to evaluate ----\n",
    "        df2 = df.copy()\n",
    "        \n",
    "        # Keep only rows that look like detections (start/end numeric)\n",
    "        df2['start_s'] = pd.to_numeric(df2.get('start', np.nan), errors='coerce')\n",
    "        df2['end_s'] = pd.to_numeric(df2.get('end', np.nan), errors='coerce')\n",
    "        df2 = df2.dropna(subset=['start_s', 'end_s'])\n",
    "\n",
    "        if df2.empty:\n",
    "            print(\"No numeric detections in `df` to evaluate.\")\n",
    "        else:\n",
    "            unique_files = sorted(df2['filename'].astype(str).unique().tolist())\n",
    "\n",
    "            gt_stem = Path(GT_FILE_PATH).stem  # \"PERU3\"\n",
    "            \n",
    "            selected_file = None\n",
    "            if EVAL_AUDIO_FILENAME is not None:\n",
    "                # user override\n",
    "                if EVAL_AUDIO_FILENAME in unique_files:\n",
    "                    selected_file = EVAL_AUDIO_FILENAME\n",
    "                else:\n",
    "                    print(f\"EVAL_AUDIO_FILENAME={EVAL_AUDIO_FILENAME!r} not found in df filenames:\")\n",
    "                    print(unique_files)\n",
    "            else:\n",
    "                # auto-pick by matching stem\n",
    "                stem_matches = [f for f in unique_files if gt_stem.lower() in Path(f).stem.lower()]\n",
    "                if len(stem_matches) == 1:\n",
    "                    selected_file = stem_matches[0]\n",
    "                elif len(unique_files) == 1:\n",
    "                    selected_file = unique_files[0]\n",
    "                else:\n",
    "                    print(\"Multiple audio files detected in results, and GT has no filename column.\")\n",
    "                    print(f\"Ground truth stem: {gt_stem!r}\")\n",
    "                    print(\"Found these result filenames:\")\n",
    "                    print(unique_files)\n",
    "                    print(\"\\nSet `EVAL_AUDIO_FILENAME = 'PERU3.wav'` (or your filename) in this cell and rerun.\")\n",
    "\n",
    "            if selected_file is None:\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"\\nEvaluating only this file: {selected_file}\")\n",
    "                det_df = df2[df2['filename'].astype(str) == selected_file].copy()\n",
    "\n",
    "                # ---- Load GT ----\n",
    "                gt_df = pd.read_csv(GT_FILE_PATH, sep='\\t')\n",
    "\n",
    "                # Map GT species -> model class labels\n",
    "                SPECIES_MAP = {\n",
    "                    'Pipistrellus tenuis': 'pip-tenuisbat-species',\n",
    "                    'Pipistrellus ceylonicus': 'pip-ceylonicusbat-species',\n",
    "                }\n",
    "\n",
    "                if 'Species' not in gt_df.columns:\n",
    "                    raise ValueError(\"'Species' column not found in ground truth file\")\n",
    "                if 'Begin Time (s)' not in gt_df.columns or 'End Time (s)' not in gt_df.columns:\n",
    "                    raise ValueError(\"Ground truth must have 'Begin Time (s)' and 'End Time (s)'\")\n",
    "\n",
    "                gt_df['Species_Mapped'] = gt_df['Species'].astype(str).str.strip().map(SPECIES_MAP)\n",
    "                gt_valid = gt_df.dropna(subset=['Species_Mapped']).copy()\n",
    "                gt_valid['gt_start_s'] = pd.to_numeric(gt_valid['Begin Time (s)'], errors='coerce')\n",
    "                gt_valid['gt_end_s'] = pd.to_numeric(gt_valid['End Time (s)'], errors='coerce')\n",
    "                gt_valid = gt_valid.dropna(subset=['gt_start_s', 'gt_end_s'])\n",
    "\n",
    "                print(f\"GT rows (mapped): {len(gt_valid)}\")\n",
    "                print(f\"Detections rows:   {len(det_df)}\")\n",
    "\n",
    "                # ---- Overlap match helper ----\n",
    "                def overlaps(a_start, a_end, b_start, b_end):\n",
    "                    return max(a_start, b_start) < min(a_end, b_end)\n",
    "\n",
    "                # Metrics\n",
    "                tp = 0\n",
    "                fn = 0\n",
    "                misclassified = 0\n",
    "                fp = 0\n",
    "\n",
    "                matched_det_indices = set()\n",
    "\n",
    "                # Recall side: for each GT event, did we predict the right species?\n",
    "                for _, gt_row in gt_valid.iterrows():\n",
    "                    gt_start = float(gt_row['gt_start_s'])\n",
    "                    gt_end = float(gt_row['gt_end_s'])\n",
    "                    gt_label = gt_row['Species_Mapped']\n",
    "\n",
    "                    candidates = []\n",
    "                    for i, det_row in det_df.iterrows():\n",
    "                        det_start = float(det_row['start_s'])\n",
    "                        det_end = float(det_row['end_s'])\n",
    "                        if overlaps(gt_start, gt_end, det_start, det_end):\n",
    "                            candidates.append((i, det_row))\n",
    "\n",
    "                    if not candidates:\n",
    "                        fn += 1\n",
    "                        continue\n",
    "\n",
    "                    # Ignore non-claims for species\n",
    "                    species_preds = [(i, r) for (i, r) in candidates if r['prediction'] not in ['No Detection', 'Noise/LowEnergy', 'Uncertain']]\n",
    "                    if not species_preds:\n",
    "                        fn += 1\n",
    "                        continue\n",
    "\n",
    "                    # If any overlapping prediction equals GT label -> TP else Misclassified\n",
    "                    if any(r['prediction'] == gt_label for _, r in species_preds):\n",
    "                        tp += 1\n",
    "                    else:\n",
    "                        misclassified += 1\n",
    "\n",
    "                    for i, _ in species_preds:\n",
    "                        matched_det_indices.add(i)\n",
    "\n",
    "                # Precision side: any species claim not matched to any GT overlap is FP\n",
    "                for i, det_row in det_df.iterrows():\n",
    "                    if det_row['prediction'] in ['No Detection', 'Noise/LowEnergy', 'Uncertain']:\n",
    "                        continue\n",
    "\n",
    "                    # If this detection overlaps ANY GT event, we don't count as FP (it was \"about\" some GT)\n",
    "                    det_start = float(det_row['start_s'])\n",
    "                    det_end = float(det_row['end_s'])\n",
    "\n",
    "                    has_any_gt_overlap = False\n",
    "                    for _, gt_row in gt_valid.iterrows():\n",
    "                        if overlaps(float(gt_row['gt_start_s']), float(gt_row['gt_end_s']), det_start, det_end):\n",
    "                            has_any_gt_overlap = True\n",
    "                            break\n",
    "\n",
    "                    if not has_any_gt_overlap:\n",
    "                        fp += 1\n",
    "\n",
    "                # Report\n",
    "                precision = tp / (tp + fp + misclassified) if (tp + fp + misclassified) > 0 else 0.0\n",
    "                recall = tp / (tp + fn + misclassified) if (tp + fn + misclassified) > 0 else 0.0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "                print(\"\\n\" + \"=\" * 40)\n",
    "                print(\"EVALUATION RESULTS\")\n",
    "                print(\"=\" * 40)\n",
    "                print(f\"File evaluated: {selected_file}\")\n",
    "                print(f\"True Positives (Correct): {tp}\")\n",
    "                print(f\"False Negatives (Missed): {fn}\")\n",
    "                print(f\"Misclassified:            {misclassified}\")\n",
    "                print(f\"False Positives:          {fp}\")\n",
    "                print(\"-\" * 40)\n",
    "                print(f\"Precision: {precision:.2%}\")\n",
    "                print(f\"Recall:    {recall:.2%}\")\n",
    "                print(f\"F1 Score:  {f1:.2%}\")\n",
    "                print(\"=\" * 40)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
